{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import jiagu\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "pandarallel.initialize(nb_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 对新闻文本数据进行预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "合计获取到502550条新闻文本数据\n"
     ]
    }
   ],
   "source": [
    "# 获得新闻文本数据\n",
    "# 获取新闻文本数据文件路径\n",
    "news_path_ori = './news/data'\n",
    "news_data = []\n",
    "# 只提取跟评论数据相同时间段的新闻文本数据\n",
    "for i in os.listdir(news_path_ori):\n",
    "    news_path = os.path.join(news_path_ori, i)\n",
    "    with open(news_path, 'r', encoding='utf-8') as f:\n",
    "        datas = json.load(f)\n",
    "        for data in datas:\n",
    "            dic = {}\n",
    "            dic['新闻发布日期'] = '2020-' + data['time'].split()[0]\n",
    "            dic['新闻发布时间'] = data['time'].split()[1]\n",
    "            dic['新闻url'] = data['url']\n",
    "            dic['新闻标题'] = data['meta']['title'].replace(' ', '')\n",
    "            dic['新闻正文'] = data['meta']['content'].replace(' ', '')\n",
    "            dic['新闻描述'] = data['meta']['description'].replace(' ', '')\n",
    "            dic['新闻关键词'] = ' '.join(data['meta']['keyword'])\n",
    "            dic['新闻类型'] = data['meta']['type'].replace(' ', '')\n",
    "            news_data.append(dic)\n",
    "print('合计获取到{}条新闻文本数据'.format(len(news_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "去重后，剩余499649条新闻文本数据\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>新闻发布日期</th>\n",
       "      <th>新闻发布时间</th>\n",
       "      <th>新闻url</th>\n",
       "      <th>新闻标题</th>\n",
       "      <th>新闻正文</th>\n",
       "      <th>新闻描述</th>\n",
       "      <th>新闻关键词</th>\n",
       "      <th>新闻类型</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-02-27</td>\n",
       "      <td>00:00</td>\n",
       "      <td>https://finance.sina.com.cn/money/future/roll/...</td>\n",
       "      <td>弱成本&amp;amp;弱供需PTA顺势下行</td>\n",
       "      <td>热点栏目自选股数据中心行情中心资金流向模拟交易客户端原标题：【PTA】弱成本&amp;弱供需PTA顺...</td>\n",
       "      <td>弱成本&amp;amp;弱供需PTA顺势下行</td>\n",
       "      <td>PTA 下行</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-02-27</td>\n",
       "      <td>02:32</td>\n",
       "      <td>https://finance.sina.com.cn/chanjing/cyxw/2020...</td>\n",
       "      <td>英媒：中国养蜂人遭“封锁”新冠病毒“蜇疼”蜂蜜生产国</td>\n",
       "      <td>原标题：英媒：新冠病毒“蜇疼”中国养蜂人路透社2月26日文章，原题：随着中国养蜂人遭“封锁”...</td>\n",
       "      <td>英媒：中国养蜂人遭“封锁”新冠病毒“蜇疼”蜂蜜生产国</td>\n",
       "      <td>疫情 新冠肺炎</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-02-27</td>\n",
       "      <td>02:32</td>\n",
       "      <td>https://finance.sina.com.cn/world/gjjj/2020-02...</td>\n",
       "      <td>美媒：中国超级富豪人数超过美印之和</td>\n",
       "      <td>原标题：美媒：中国超级富豪人数超过美印之和雅虎财经网2月26日文章，原题：胡润全球富豪榜：中...</td>\n",
       "      <td>美媒：中国超级富豪人数超过美印之和</td>\n",
       "      <td></td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-02-27</td>\n",
       "      <td>02:32</td>\n",
       "      <td>https://finance.sina.com.cn/world/gjjj/2020-02...</td>\n",
       "      <td>英媒：疫情导致中国的野生动物养殖面临困境</td>\n",
       "      <td>原标题：英媒：中国的野生动物养殖面临困境英国《卫报》2月25日文章，原题：因新冠病毒导致的野...</td>\n",
       "      <td>英媒：疫情导致中国的野生动物养殖面临困境</td>\n",
       "      <td>野生动物 新冠肺炎</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-02-27</td>\n",
       "      <td>02:33</td>\n",
       "      <td>https://news.sina.com.cn/w/2020-02-27/doc-iimx...</td>\n",
       "      <td>印度：停职官员要坐“反省椅”等待最后“判决”</td>\n",
       "      <td>原标题：印度：停职官员要坐“反省椅”[环球时报综合报道]“停职不是带薪假。”印度中央邦的瓜廖...</td>\n",
       "      <td>停职官员要坐“反省椅”</td>\n",
       "      <td>印度</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       新闻发布日期 新闻发布时间                                              新闻url  \\\n",
       "0  2020-02-27  00:00  https://finance.sina.com.cn/money/future/roll/...   \n",
       "1  2020-02-27  02:32  https://finance.sina.com.cn/chanjing/cyxw/2020...   \n",
       "2  2020-02-27  02:32  https://finance.sina.com.cn/world/gjjj/2020-02...   \n",
       "3  2020-02-27  02:32  https://finance.sina.com.cn/world/gjjj/2020-02...   \n",
       "4  2020-02-27  02:33  https://news.sina.com.cn/w/2020-02-27/doc-iimx...   \n",
       "\n",
       "                         新闻标题  \\\n",
       "0          弱成本&amp;弱供需PTA顺势下行   \n",
       "1  英媒：中国养蜂人遭“封锁”新冠病毒“蜇疼”蜂蜜生产国   \n",
       "2           美媒：中国超级富豪人数超过美印之和   \n",
       "3        英媒：疫情导致中国的野生动物养殖面临困境   \n",
       "4      印度：停职官员要坐“反省椅”等待最后“判决”   \n",
       "\n",
       "                                                新闻正文  \\\n",
       "0  热点栏目自选股数据中心行情中心资金流向模拟交易客户端原标题：【PTA】弱成本&弱供需PTA顺...   \n",
       "1  原标题：英媒：新冠病毒“蜇疼”中国养蜂人路透社2月26日文章，原题：随着中国养蜂人遭“封锁”...   \n",
       "2  原标题：美媒：中国超级富豪人数超过美印之和雅虎财经网2月26日文章，原题：胡润全球富豪榜：中...   \n",
       "3  原标题：英媒：中国的野生动物养殖面临困境英国《卫报》2月25日文章，原题：因新冠病毒导致的野...   \n",
       "4  原标题：印度：停职官员要坐“反省椅”[环球时报综合报道]“停职不是带薪假。”印度中央邦的瓜廖...   \n",
       "\n",
       "                         新闻描述      新闻关键词  新闻类型  \n",
       "0          弱成本&amp;弱供需PTA顺势下行     PTA 下行  news  \n",
       "1  英媒：中国养蜂人遭“封锁”新冠病毒“蜇疼”蜂蜜生产国    疫情 新冠肺炎  news  \n",
       "2           美媒：中国超级富豪人数超过美印之和             news  \n",
       "3        英媒：疫情导致中国的野生动物养殖面临困境  野生动物 新冠肺炎  news  \n",
       "4                 停职官员要坐“反省椅”         印度  news  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 转成DataFrame格式，剔除content无空的数据，并进行去重\n",
    "news_data_df = pd.DataFrame(news_data)\n",
    "news_data_df = news_data_df.fillna('')\n",
    "news_data_df = news_data_df[news_data_df['新闻正文']!='']\n",
    "news_data_df = news_data_df[news_data_df['新闻url']!=''] \n",
    "news_data_df.drop_duplicates(subset=['新闻url'], inplace=True)\n",
    "print('去重后，剩余{}条新闻文本数据'.format(len(news_data_df)))\n",
    "news_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1为没有关键词的新闻提取关键词\n",
    "- 对关键词为空的新闻正文，使用jiagu的findword接口进行新词发现\n",
    "- 将新词加载到jieba的用户词典\n",
    "- 使用jiagu的keywords接口进行关键词的提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "缺失关键词的数据量为: 130451\n"
     ]
    }
   ],
   "source": [
    "# 查看有多少新闻的关键词为空\n",
    "print('缺失关键词的数据量为:', len(news_data_df[news_data_df['新闻关键词']=='']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新词发现\n",
    "no_keyword_news_data = list(news_data_df[news_data_df['新闻关键词']=='']['新闻正文'])\n",
    "no_keyword_news_data = list(news_data_df[news_data_df['新闻关键词']=='']['新闻正文'])\n",
    "with open('news/preprocess_data/no_keywords_text.txt', 'w', encoding='utf8') as f:\n",
    "    for i in range(len(no_keyword_news_data)):\n",
    "        f.write(no_keyword_news_data[i])\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "jiagu.findword('news/preprocess_data/no_keywords_text.txt', 'news/preprocess_data/new_words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625\n"
     ]
    }
   ],
   "source": [
    "# 读取jieba的词典数据\n",
    "jieba_lst = []\n",
    "with open('/Users/apple/opt/anaconda3/envs/pytorch/lib/python3.7/site-packages/jieba/dict.txt', 'r') as f:\n",
    "    for i in f.readlines():\n",
    "        jieba_lst.append(i.strip().split(' ')[0])\n",
    "jieba_lst = set(jieba_lst)\n",
    "# 取获得前3000个新词\n",
    "new_words = []\n",
    "with open('news/preprocess_data/new_words.txt', 'r') as f:\n",
    "    for i in f.readlines():\n",
    "        new_words.append(i.strip().split('\\t')[0])\n",
    "new_words = new_words[:3000]\n",
    "new_words = set(new_words)\n",
    "# 把新词中在jieba词典里的词剔除\n",
    "diff = []\n",
    "for i in new_words:\n",
    "    if i not in jieba_lst:\n",
    "        diff.append(i)\n",
    "print(len(diff))\n",
    "# 将获得新词保存\n",
    "with open('./news/preprocess_data/new_word_dict.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in diff:\n",
    "        f.write(i + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "受\n",
      "新冠疫情\n",
      "影响\n",
      "，\n",
      "宁德时代\n",
      "董事长\n",
      "说\n",
      "，\n",
      "本次\n",
      "经济\n",
      "下行\n",
      "已成定局\n"
     ]
    }
   ],
   "source": [
    "# 对发现的新词进行筛选后，得到261个新词，导入jieba的用户词典中\n",
    "jieba.load_userdict('./news/preprocess_data/new_word_dict.txt')\n",
    "# 测试\n",
    "for word in jieba.cut('受新冠疫情影响，宁德时代董事长说，本次经济下行已成定局'):\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    369198.000000\n",
       "mean          7.586447\n",
       "std           4.132212\n",
       "min           1.000000\n",
       "25%           4.000000\n",
       "50%           7.000000\n",
       "75%          10.000000\n",
       "max          56.000000\n",
       "Name: 新闻关键词, dtype: float64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看存在的关键词的数量分布，发现中位数以及平均数为7，因此选择7作为关键词数量进行提取\n",
    "news_data_df[news_data_df['新闻关键词']!='']['新闻关键词'].apply(lambda x: len(x)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选择数量7进行关键词数量进行提取\n",
    "# 加载停用词表\n",
    "stop_words = set()\n",
    "\n",
    "with open('utils/stop_words.txt', 'r', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        word = line.strip()\n",
    "        if not word:\n",
    "            continue\n",
    "        stop_words.add(word)\n",
    "\n",
    "\n",
    "def textrank_keyword(text, top_k=7):\n",
    "    '''使用jiagu的textrank接口进行关键词提取'''\n",
    "    text = text.replace(' ', '')\n",
    "    text = [word for word in jieba.cut(text) if word not in stop_words]\n",
    "    keywords = jiagu.keywords(''.join(text), top_k)\n",
    "    keywords = ' '.join(keywords)\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# 使用pandarallel进行加速\n",
    "news_data_df.loc[news_data_df['新闻关键词']=='', '新闻关键词'] = \\\n",
    "                news_data_df[news_data_df['新闻关键词']=='']['新闻正文'].parallel_apply(textrank_keyword)\n",
    "print(len(news_data_df[news_data_df['新闻关键词']=='']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将新闻发布日期、新闻发布url、新闻关键词进行保存\n",
    "news_data_save = news_data_df[['新闻发布日期', '新闻url', '新闻标题', '新闻关键词']]\n",
    "news_data_save.to_csv('./news/preprocess_data/新闻正文关键词.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查看目前所有关键词，并对出现较高的那1000个关键词提取出来进行人工筛查，将一些有问题的关键词剔除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "top1000_keyword = {}\n",
    "for keywords in news_data_save['新闻关键词']:\n",
    "    for i in keywords.split():\n",
    "        top1000_keyword[i] = top1000_keyword.get(i, 0) + 1\n",
    "top1000_keyword = sorted(top1000_keyword.items(), key=lambda d: d[1], reverse=True)[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./news/preprocess_data/关键词-待筛选.txt', 'w', encoding='utf8') as f:\n",
    "    for i in top1000_keyword:\n",
    "        f.write('{}\\t{}\\n'.format(i[0], i[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 经过筛选后，有749个关键词保留下来\n",
    "keyword_need = []\n",
    "with open('./news/preprocess_data/关键词-已筛选.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        keyword_need.append(line.split('\\t')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "需要剔除的关键词有330个\n"
     ]
    }
   ],
   "source": [
    "keyword_not_need = np.setdiff1d(np.array([word[0] for word in top1000_keyword]), np.array(keyword_need))\n",
    "print('需要剔除的关键词有{}个'.format(len(keyword_not_need)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./news/preprocess_data/关键词-待剔除.txt', 'w') as f:\n",
    "    for i in keyword_not_need:\n",
    "        f.write('{}\\n'.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将news_data_save中的需要剔除的关键词剔除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/opt/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "def filter_keyword(keywords):\n",
    "    keywords = keywords.split()\n",
    "    keywords = np.setdiff1d(np.array(keywords), np.array(keyword_not_need))\n",
    "    return ' '.join(keywords)\n",
    "\n",
    "news_data_save['新闻关键词'] = news_data_save['新闻关键词'].map(filter_keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 对新闻关键词计算权值、排序，并制作词云图所需数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/opt/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# 考虑到每条新闻可能存在多个关键词，如果直接对关键词进行计数，会使得一些新闻关键词的权值降低，不能更好的反映这个关键词的重要性\n",
    "# 因此我们设定每条新闻的关键词的总权值为1，每条新闻中每个关键词的权值为1/该条新闻的关键词数量\n",
    "# 另外，按照日期进行关键词统计\n",
    "date_num = news_data_save['新闻发布日期'].unique()\n",
    "news_data_save['关键词长度'] = news_data_save['新闻关键词'].apply(lambda x: len(x.split()))\n",
    "keywords_dict_date = {}\n",
    "for news_date in date_num:\n",
    "    keywords_dict_date[news_date] = {}\n",
    "    news_data_day = news_data_save[news_data_save['新闻发布日期']==news_date]\n",
    "    for i in range(len(news_data_day['新闻关键词'])):\n",
    "        keywords = news_data_day['新闻关键词'].iloc[i].split()\n",
    "        keyword_len = news_data_day['关键词长度'].iloc[i]\n",
    "        for keyword in keywords:\n",
    "            keywords_dict_date[news_date][keyword] = keywords_dict_date[news_date].get(keyword, 0) + 1/keyword_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对每个日期的关键词按照权重进行倒序排序\n",
    "for news_date in keywords_dict_date.keys():\n",
    "    keywords_dict_date[news_date] = sorted(keywords_dict_date[news_date].items(), \n",
    "                                           key=lambda d: d[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 由于每个日期的每个关键词的权重都不一样，需要进行归一化操作\n",
    "def normalization_keyword(keyword_lst):\n",
    "    # 以最大值作为1值，其他值 / 最大值 * 10\n",
    "    rank_1 = keyword_lst[0][1]\n",
    "    keyword_lst_norm = []\n",
    "    for i in range(len(keyword_lst)):\n",
    "        keyword_lst_norm.append((keyword_lst[i][0], keyword_lst[i][1] / rank_1 * 10))\n",
    "    return keyword_lst_norm\n",
    "\n",
    "for news_date in keywords_dict_date.keys():\n",
    "    keywords_dict_date[news_date] = normalization_keyword(keywords_dict_date[news_date])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 仅提取每天前10个权重值的关键词进行保存，以便于绘制词云图\n",
    "top_k=10\n",
    "keywords_dict_date_topk = {}\n",
    "for news_date in keywords_dict_date.keys():\n",
    "    keywords_dict_date_topk[news_date] = [{'name': data[0], 'value': data[1]} for data in keywords_dict_date[news_date][:top_k]]\n",
    "\n",
    "with open('./report/echarts图表/js/news_keywords_top{}.js'.format(top_k), 'w', encoding='utf8') as f:\n",
    "    f.write('const keywords = ')\n",
    "    f.write(str(keywords_dict_date_topk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取日期，以便于绘制词云图\n",
    "with open('./report/echarts图表/js/news_date.js', 'w', encoding='utf8') as f:\n",
    "    f.write('const news_date = ')\n",
    "    f.write(str(sorted(list(keywords_dict_date.keys()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 可以看到在2020-01-19之前，新闻关键词基本以伊朗、特朗普为主，说明当时的新闻以国际形势为主\n",
    "- 但是从2020-01-19开始，关于疫情、湖北、新冠的关键词开始慢慢增多，并开始占据较大范围\n",
    "- 通过新闻关键词，深刻反映了那段时期，中国新闻报道的主流趋势\n",
    "- 具体见`report/echarts图表/新闻文本关键词词云.html`\n",
    "![](report/gif/新闻文本关键词词云.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e01e9ef47000516f4422d4bd60458906d374317ff2d90b45c94dbf9e098ea38c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('pytorch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
